## Directory Tree

├── chat_loop_bot.py
├── focus
    ├── create_focus.py
    └── focus.json
├── future_tools_for_future _integration
    ├── tool_create_memory.py
    └── tool_update_internal_state.py
├── images
├── inner_brain_settings
    └── internal_state.json
├── keys.py
├── readMe
├── tools
    ├── ai
        ├── tool_AI_REASONING.py
        ├── tool_update_focus.py
        └── __pycache__
    ├── memory
    ├── os
        ├── tool_execute_script.py
        ├── tool_get_directory_structure.py
        ├── tool_read_from_file.py
        ├── tool_save_to_file.py
        └── __pycache__
    └── web
        ├── tool_get_duckduckgo_links.py
        ├── tool_save_image_from_url.py
        ├── tool_scrape_url.py
        └── __pycache__
├── TOOL_MANAGER.py
└── __pycache__

## Summary of 'C:\Users\DELL\Desktop\openAIF_frontend\Gemini_george_bot\GEORGE_bot_v0'

File: chat_loop_bot.py (C:\Users\DELL\Desktop\openAIF_frontend\Gemini_george_bot\GEORGE_bot_v0\chat_loop_bot.py)
Content (10461 characters):
import time
import google.generativeai as genai
import json
from typing import List, Dict, Optional
import logging
import os
from TOOL_MANAGER import ToolManager  # Assuming you have a TOOL_MANAGER.py file

tool_manager = ToolManager("tools")

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Replace with your actual API key
from keys import googleKey  # Assuming you have a separate file for your API key

API_KEY = googleKey
genai.configure(api_key=API_KEY)

from google.generativeai.types import HarmCategory, HarmBlockThreshold

safety_settings = {
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
}


class Color:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'


def print_colored(color: str, text: str):
    print(color + str(text) + Color.ENDC)


def load_focus_data(focus_file_path: str) -> str:
    """Loads focus data as a string from the specified file."""
    try:
        with open(focus_file_path, "r") as f:
            focus_data = f.read()
        return focus_data
    except FileNotFoundError:
        print_colored(Color.WARNING, f"Focus file not found: {focus_file_path}")
        return ""
    except Exception as e:
        print_colored(Color.FAIL, f"Error loading focus file: {e}")
        return ""


def extract_text_from_response(response) -> str:
    """Extracts text content from model response with error handling."""
    try:
        extracted_text = ""
        if response.candidates:
            for candidate in response.candidates:
                for part in candidate.content.parts:
                    extracted_text += part.text
        return extracted_text.strip()
    except Exception as e:
        logger.error(f"Error extracting text from response: {e}")
        return "Error processing response"


def handle_tool_calls(response):
    """Interprets and executes function calls from model response with enhanced error handling."""
    results = []
    try:
        if response.candidates:
            for candidate in response.candidates:
                if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                    for part in candidate.content.parts:
                        function_call = getattr(part, 'function_call', None)
                        if function_call:
                            print_colored(Color.OKBLUE, "---------------TOOL EXECUTION-------------------")
                            tool_name = function_call.name
                            tool_function = tool_manager.get_tool_function(tool_name)

                            if tool_function:
                                function_args = {
                                    arg_name: arg_value
                                    for arg_name, arg_value in function_call.args.items()
                                }

                                print(f"Executing: {Color.OKGREEN}{tool_name}{Color.ENDC}")
                                print("Arguments:")
                                for key, value in function_args.items():
                                    print(f"        {Color.OKCYAN}{key}{Color.ENDC}: {value}")

                                try:
                                    result = tool_function(**function_args)
                                    result_str = f"Tool {Color.OKGREEN}{tool_name}{Color.ENDC} executed successfully:\n{result}"
                                    results.append(result_str)
                                    print_colored(Color.OKGREEN, result_str)
                                except Exception as e:
                                    error_msg = f"Error executing {tool_name}: {str(e)}"
                                    logger.error(error_msg)
                                    results.append(error_msg)
                                    print_colored(Color.FAIL, error_msg)
                            else:
                                error_msg = f"Tool '{tool_name}' not found in available tools"
                                logger.warning(error_msg)
                                results.append(error_msg)
    except Exception as e:
        error_msg = f"Error interpreting tool calls: {str(e)}"
        logger.error(error_msg)
        results.append(error_msg)
    return results


def process_turn(user_input: str) -> None:
    """Handles a single turn in the conversation."""
    global conversation_history, current_turn, memory
    try:
        # Reset the current turn
        current_turn = []

        # Stage 1: Input/Reasoning Model
        conversation_history.append(f"User: {user_input}")
        time.sleep(1)
        combined_prompt = f"""

            Conversation:
            {conversation_history[-1]} 
            Focus: {load_focus_data('focus/focus.json')} What are the possible actions to take next? 
            Available Tools:
            {tool_manager.get_short_tool_descriptions()}
        """
        try:
            response_input = input_model.generate_content(combined_prompt)
            print(f"Input/Reasoning Model Response: {response_input}")

            conversation_history.append(f" {response_input}")
            text_extracted_response = extract_text_from_response(response_input)

            tool_results = handle_tool_calls(response_input)
            print(f"Extracted Text: {text_extracted_response}")
            print(f"Tool Results: {tool_results}")
            conversation_history.append(f"obtained tool Results in conversation context: {tool_results}")
            current_turn.extend(tool_results)
        except Exception as E:
            print_colored(Color.FAIL, f"Error generating content from Input/Reasoning Model: {E}")

        # Stage 2: Action Taker
        time.sleep(1)
        try:
            conversation_history.append("take  next  step  accoording  to   logical execution of  steps  ")
            response_action_taker = action_taker_model.generate_content(conversation_history)
            print(f"Action Taker Model Response: {response_action_taker}")

            conversation_history.append(f"Action Taker Model Response: {response_action_taker}")
            text_extracted_response = extract_text_from_response(response_action_taker)
            print(f"Extracted Text: {text_extracted_response}")

            tool_results = handle_tool_calls(response_action_taker)
            print(f"Tool Results: {tool_results}")
            conversation_history.append(f"Tool Results: {tool_results}")
            current_turn.extend(tool_results)
        except Exception as E:
            print_colored(Color.FAIL, f"Error generating content from Action Taker Model: {E}")

        # Stage 3: Evaluator Model
        time.sleep(3)
        try:
            conversation_history.append(" update  your  focus , you must  describe   what  has  been accomplished ")
            response_evaluator = evaluator_model.generate_content(conversation_history)
            print(f"Evaluator Model Response: {response_evaluator}")
            conversation_history.append(f"Evaluator Model Response: {response_evaluator}")
            text_extracted_response = extract_text_from_response(response_evaluator)
            print(f"Extracted Text: {text_extracted_response}")

            tool_results = handle_tool_calls(response_evaluator)
            print(f"Tool Results: {tool_results}")
            conversation_history.extend(tool_results)
        except Exception as E:
            print_colored(Color.FAIL, f"Error generating content from Evaluator Model: {E}")

    except Exception as e:
        print(e)
        for entry in conversation_history:
            print(entry)
        time.sleep(5)


# Initialize conversation history
conversation_history = []
current_turn = []
memory = {}
availbe_tools = tool_manager.get_short_tool_descriptions()
print("************************************************************************************")
print(availbe_tools)
# System instructions for each model
input_system_instruction = f"""
You are a helpful AI assistant with a focus on completing the current task. 
You can update your internal focus.
You have access to these tools: {availbe_tools}
"""
print()
action_taking_system_instruction = """
You are a tool execution expert. Do not hallucinate! 
Execute the following actions based on the identified focus and tool suggestions: [List the actions from the input model].

"""

evaluation_system_instruction = """
Summarize results, identify issues, and suggest improvements.
Explain your reasoning. Based on the results, update your focus  .
"""

# Initialize models
try:
    input_model = genai.GenerativeModel(
        model_name='gemini-1.5-flash-latest',
        safety_settings=safety_settings,
        system_instruction=input_system_instruction,
        tools=tool_manager.load_tools_of_type("all")
    )
except Exception as E:
    print_colored(Color.FAIL, f"Error initializing input_model: {E}")

try:
    action_taker_model = genai.GenerativeModel(
        model_name='gemini-1.5-flash-latest',
        safety_settings=safety_settings,
        system_instruction=action_taking_system_instruction,
        tools=tool_manager.load_tools_of_type("all")
    )
except Exception as E:
    print_colored(Color.FAIL, f"Error initializing action_taker_model: {E}")

try:
    evaluator_model = genai.GenerativeModel(
        model_name='gemini-1.5-flash-latest',
        safety_settings=safety_settings,
        system_instruction=evaluation_system_instruction,
        tools=tool_manager.load_tools_of_type("all")
    )
except Exception as E:
    print_colored(Color.FAIL, f"Error initializing evaluator_model: {E}")



# Main Loop:
# Initial user input
user_input = input("user_input: ")
process_turn(user_input)

while True:
    # Ask for user input after every 3 turns
    for _ in range(3):
        process_turn("")  # Process empty input for the next 3 turns

    user_input = input("user_input: ")  # Get user input after 3 turns
    process_turn(user_input)

File: keys.py (C:\Users\DELL\Desktop\openAIF_frontend\Gemini_george_bot\GEORGE_bot_v0\keys.py)
Content (51 characters):
googleKey='AIzaSyDuqD6MksWRqXB0VgHC22xSMn41BaP0reA'

File: readMe (C:\Users\DELL\Desktop\openAIF_frontend\Gemini_george_bot\GEORGE_bot_v0\readMe)
Content (78 characters):
you allso  need  to add  your  api  google  ke  in tools  tool_AI_REASONING.py

File: TOOL_MANAGER.py (C:\Users\DELL\Desktop\openAIF_frontend\Gemini_george_bot\GEORGE_bot_v0\TOOL_MANAGER.py)
Content (7256 characters):
import os
import importlib
from typing import Dict, Callable, List, Any
import logging
import inspect

# Set up logging (optional, but recommended)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class Tool:
    """Represents a tool that can be used by the AI agent."""

    def __init__(self, name: str, function: Callable, description: str, arguments: Dict[str, str], tool_type: str):
        """
        Initializes a Tool object.

        Args:
            name: The name of the tool.
            function: The callable function that implements the tool.
            description: A brief description of the tool's functionality.
            arguments: A dictionary mapping argument names to their descriptions.
            tool_type: The type of the tool (e.g., 'os', 'web', 'focus').
        """
        self.name = name
        self.function = function
        self.description = description
        self.arguments = arguments
        self.tool_type = tool_type

    def __repr__(self):
        """Returns a string representation of the Tool object."""
        return f"Tool(name='{self.name}', function={self.function.__name__}, description='{self.description}', arguments={self.arguments}, tool_type='{self.tool_type}')"


class ToolManager:
    """Manages and provides access to tools."""

    def __init__(self, tools_folder: str):
        """
        Initializes the ToolManager with the path to the tools folder.

        Args:
            tools_folder: The path to the directory containing tool files.
        """
        self.tools_folder = tools_folder
        self.tools = {}  # Dictionary to store Tool objects
        self.load_tools()

    def load_tools(self):
        """Loads tools from files in the specified tools folder."""
        logger.info(f"Loading tools from: {self.tools_folder}")
        for root, _, files in os.walk(self.tools_folder):
            for file in files:
                if file.endswith(".py"):
                    # Extract tool name from file name
                    tool_name_from_file = file[:-3]  # Remove .py extension
                    module_path = os.path.join(root, file)

                    # Import the module
                    try:
                        spec = importlib.util.spec_from_file_location(tool_name_from_file, module_path)
                        module = importlib.util.module_from_spec(spec)
                        spec.loader.exec_module(module)
                    except Exception as e:
                        logger.error(f"Error loading tool file '{file}': {e}")
                        continue

                    # Iterate through module attributes to find tool functions
                    for attr_name in dir(module):
                        attr = getattr(module, attr_name)
                        if callable(attr) and attr_name.startswith("tool_"):  # Check if it's a callable and starts with "tool_"

                            tool_name = attr_name  # Use the attribute name as the tool name
                            relative_path = os.path.relpath(module_path, self.tools_folder)  # Get the relative path to the tool file

                            # Get the short description for the tool from the module
                            tool_description = getattr(module, f"{tool_name}_short_description", f"Tool for {tool_name}")

                            # Get the tool arguments using inspect
                            tool_arguments = {}
                            sig = inspect.signature(attr)  # Get the function signature
                            for param_name, param in sig.parameters.items():
                                # Get the parameter type using annotation
                                tool_arguments[param_name] = param.annotation.__name__ if param.annotation != inspect._empty else "Any"

                            # Get the tool type from the module (optional, defaults to 'unknown')
                            tool_type = getattr(module, 'tool_type_for_TOOL_MANAGER', 'unknown')

                            # Create and store the Tool object
                            self.tools[tool_name] = Tool(tool_name, attr, tool_description, tool_arguments, tool_type)

                            # Log the discovered tool
                            logger.info(f"Discovered tool: {tool_name} (Type: {tool_type})")
                            print(f"  - {tool_name} - {tool_description}")  # Print the tool information
                            logger.debug(f"Tool description: {tool_description}")
                            logger.debug(f"Tool arguments: {tool_arguments}")  # Log the tool arguments

    def get_tool_function(self, function_name: str) -> Callable:
        """Returns the callable object for the given function name."""
        tool = self.tools.get(function_name)
        if tool:
            return tool.function
        else:
            return None

    def get_all_tools(self) -> List[Tool]:
        """Returns a list of all loaded tools."""
        return list(self.tools.values())

    def get_tools_by_type(self, tool_type: str) -> List[Tool]:
        """Returns a list of tools based on their type."""
        return [tool for tool in self.tools.values() if tool.tool_type == tool_type]

    def load_tools_of_type(self, tool_type: str = "all") -> List[Callable]:
        """Loads and returns a list of tool functions based on the specified type.

        Args:
            tool_type: The type of tools to load. 'all' for all tools, or a specific type like 'os', 'web', etc.

        Returns:
            A list of tool functions.
        """
        if tool_type == "all":
            return [tool.function for tool in self.tools.values()]
        else:
            return [tool.function for tool in self.tools.values() if tool.tool_type == tool_type]

    def call_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Any:
        """
        Calls the tool function with the provided arguments.

        Args:
            tool_name: The name of the tool to call.
            arguments: A dictionary of arguments to pass to the tool function.

        Returns:
            The result of the tool function call.

        Raises:
            KeyError: If the tool name is not found.
            TypeError: If the provided arguments are not valid for the tool.
        """
        tool = self.tools.get(tool_name)
        if tool is None:
            raise KeyError(f"Tool '{tool_name}' not found.")

        # Check if all required arguments are provided
        missing_args = set(tool.arguments.keys()) - set(arguments.keys())
        if missing_args:
            raise TypeError(f"Missing arguments for tool '{tool_name}': {', '.join(missing_args)}")

        # Call the tool function
        try:
            result = tool.function(**arguments)
            return result
        except Exception as e:
            raise RuntimeError(f"Error calling tool '{tool_name}': {e}")

    def get_short_tool_descriptions(self) -> Dict[str, str]:
        """Returns a dictionary of tool names and their short descriptions."""
        return {tool.name: tool.description for tool in self.tools.values()}

File: create_focus.py (C:\Users\DELL\Desktop\openAIF_frontend\Gemini_george_bot\GEORGE_bot_v0\focus\create_focus.py)
Content (1666 characters):
import json
import os


def create_focus_file(file_path="focus.json"):
    """
    Creates a new focus file with default values.

    Args:
      file_path (str): The path to the focus file.
    """

    focus_data = {
        "current_focus": "",  # Changed to empty string
        "user_goal": "",  # Changed to empty string
        "frustration": 0.2,
        "focus_strength": 0.8,
        "defocus_threshold": 0.5,
        "importance": 0.9,
        "progress": 0.1,
        "additional": "",  # Changed to empty string
        "verbose": "",  # Changed to empty string
        "tasks": {
            "current_task": "",  # Changed to empty string
            "task_list_to_accomplish_user_goal": [
                "",  # Changed to empty string
                "",  # Changed to empty string
                ""  # Changed to empty string
            ],
            "tasks_in_progress": [
                ""  # Changed to empty string
            ],
            "finished_tasks": [
                ""  # Changed to empty string
            ],
            "failed_tasks": []
        },
        "should_defocus": False,
        "obtained_data":{},
    }

    # Create the directory if it doesn't exist
    os.makedirs(os.path.dirname(file_path), exist_ok=True)

    with open(file_path, "w") as f:
        json.dump(focus_data, f, indent=4)

    print(f"Focus file created at: {file_path}")


if __name__ == "__main__":
    # Get the current working directory
    current_dir = os.getcwd()

    # Join the current directory with the desired filename to create the full path
    file_path = os.path.join(current_dir, "focus.json")

    create_focus_file(file_path)

File: focus.json (C:\Users\DELL\Desktop\openAIF_frontend\Gemini_george_bot\GEORGE_bot_v0\focus\focus.json)
Content (52 characters):
{
    "user_goal": "",
    "steps_to_achieve_goal": 

File: tool_create_memory.py (C:\Users\DELL\Desktop\openAIF_frontend\Gemini_george_bot\GEORGE_bot_v0\future_tools_for_future _integration\tool_create_memory.py)
Content (13018 characters):
tool_type_for_TOOL_MANAGER = "memory"
tool_create_memory_short_description = """Updates the focus file with new focus information."""

import time
import json
import os
import logging
import google.generativeai as genai
from typing import List, Dict, Any
from datetime import datetime
import re

# --- Color and Configuration Settings ---
BLACK = "\033[30m"
RED = "\033[31m"
GREEN = "\033[32m"
YELLOW = "\033[33m"
BLUE = "\033[34m"
MAGENTA = "\033[35m"
CYAN = "\033[36m"
WHITE = "\033[37m"
RESET = "\033[0m"
BOLD = "\033[1m"
UNDERLINE = "\033[4m"
REVERSE = "\033[7m"

# --- Global Variables ---
MEMORY_FRAME_NUMBER = 1
EDIT_NUMBER = 0
TIMESTAMP_FORMAT = '%Y-%m-%d_%H-%M'
SESSION_INFO = "Conversation"  # Renamed for clarity

# --- API Key and Configuration ---
googleKey = 'AIzaSyChx1mgNxXW4RwrnEPr3DCWvU_sQIV_4WM'  # Moved for better structure
genai.configure(api_key=googleKey)

# --- Helper Functions ---

def sanitize_href(href: str, memories_folder_path: str) -> str:
    """Sanitizes a given href string by replacing spaces with %20."""
    href = href.replace(" ", "%20")
    return href


def get_memories_folder_path() -> str:
    """Returns the absolute path to the 'memory' folder."""
    current_dir = os.path.abspath(os.path.dirname(__file__))
    memories_path = os.path.join(current_dir, "memory")
    return memories_path


def process_user_input() -> str:
    """Gets user input from the console."""
    user_input = input(f"{GREEN}Enter input: {RESET}")
    print(f"{MAGENTA}User input received: {user_input}{RESET}")
    return user_input


def call_interaction_model(user_input: str, timestamp: str) -> genai.GenerateContentResponse:
    """Calls the interaction model with the provided user input and timestamp."""
    print(f"\n{CYAN}--- Calling Interaction Model ---{RESET}")
    try:
        interaction_model = genai.GenerativeModel(
            model_name='gemini-1.5-flash-latest',
            safety_settings={'HARASSMENT': 'block_none'},
            system_instruction='You follow orders and generate creative text interactions'
        )
        chat = interaction_model.start_chat(history=[])
        response = chat.send_message(f"currentTime: {timestamp} create {user_input}")
        print(f"AI Response: {response.text}")
        return response
    except Exception as e:
        print(f"Error in Interaction Model: {e}")
        return None


def call_memory_model(loop_conversation: str) -> genai.GenerateContentResponse:
    """Calls the memory model to analyze and summarize the provided conversation loop."""
    print(f"\n{CYAN}--- Calling Memory Model ---{RESET}")
    try:
        memory_model = genai.GenerativeModel(
            model_name='gemini-1.5-flash-latest',
            safety_settings={'HARASSMENT': 'block_none'},
            system_instruction="""You are a sophisticated AI assistant helping to organize memory. 
            Analyze and summarize the provided conversation, focusing on elements that would be most useful for storing and retrieving this memory later. Don't hallucinate. 
            Use the provided JSON schema for your response and fill in all fields with relevant information.
            You can omit entries if they don't seem appropriate for memory storage and would be empty.
            Never omit the "memory_folders_storage" entry.

            **JSON Schema:** 
            {
                "naming_suggestion": {
                    "memory_frame_name": "A descriptive name for the memory frame" 
                },
                "storage": {
                    "memory_folders_storage": [
                        {
                            "folder_path": "path/to/folder", 
                            "probability": 5 
                        }
                    ]
                },
                "interaction": {
                    "interaction_type": [], 
                    "people": [], 
                    "objects": [], 
                    "animals": [], 
                    "actions": [], 
                    "observed_interactions": [] 
                },
                "impact": {
                    "obtained_knowledge": "", 
                    "positive_impact": "", 
                    "negative_impact": "", 
                    "expectations": "", 
                    "strength_of_experience": "" 
                },
                "importance": {
                    "reason": "", 
                    "potential_uses": [], 
                    "importance_level": "0-100" 
                },
                "technical_details": {
                    "problem_solved": "", 
                    "concept_definition": "", 
                    "implementation_steps": [], 
                    "tools_and_technologies": [], 
                    "example_projects": [], 
                    "best_practices": [], 
                    "common_challenges": [], 
                    "debugging_tips": [], 
                    "related_concepts": [], 
                    "resources": [], 
                    "code_examples": [] 
                }
            }

            Here you have existing folder structure for memory_folders_storage:
            memory/NewGeneratedbyAI/

            **Memory Storage Suggestions:**
            Provide your suggestions for where this memory frame should be stored using the following format within the "memory_folders_storage" field:

            * **"folder_path":** The relative path for storing the memory frame (use '/' as the path separator).
            * **"probability":** The strength of probability (from 0 to 10) that the memory frame should be stored in the suggested folder. Use a scale from 0 (least likely) to 10 (most likely) to express your confidence. 
        """
        )
        chat = memory_model.start_chat(history=[])
        create_memory_prompt = f"Loop {loop_conversation}"
        response = chat.send_message(create_memory_prompt)
        print(f"Memory Model Response:\n{response.text}")
        return response
    except Exception as e:
        print(f"Error in Memory Model: {e}")
        return None


def extract_entries_smart(response_message: str) -> List[Dict[str, Any]]:
    """Extracts structured JSON entries from the AI response message."""
    print("\n--- Extracting Structured Entries ---")
    entries = []
    json_match = re.search(r"```json\n(.*?)\n```", response_message, re.DOTALL)
    if json_match:
        print("Found JSON data in the response.")
        try:
            json_data = json_match.group(1)
            print("Parsing JSON data...")
            response_data = json.loads(json_data)
            print("JSON data parsed successfully.")

            if isinstance(response_data, list):
                entries.extend(response_data)
            elif isinstance(response_data, dict):
                entries.append(response_data)
            else:
                print(f"Warning: Unexpected data type: {type(response_data)}")
                print("Skipping data.")

        except json.JSONDecodeError as e:
            print(f"Error: Invalid JSON in the AI response: {e}")
        except Exception as e:
            print(f"Error extracting entry: {e}")
    return entries


def save_to_file(content: str, file_name: str, file_path: str, encoding: str = 'utf-8', create_folders: bool = True) -> \
Dict[str, Any]:
    """Saves content to a file with error handling and folder creation."""
    logging.info("Entering: save_to_file")
    full_path = os.path.join(file_path, file_name)

    try:
        if create_folders:
            os.makedirs(file_path, exist_ok=True)
        with open(full_path, 'w', encoding=encoding) as f:
            f.write(content)
        success_message = f"File saved successfully at: {full_path}"
        logging.info(success_message)
        return {"status": "success", "message": success_message, "file_path": full_path}
    except IOError as e:
        error_message = f"IOError: Failed to save file: {str(e)}"
        logging.error(error_message)
        return {"status": "failure", "message": error_message}
    except Exception as e:
        error_message = f"Unexpected error: Failed to save file: {str(e)}"
        logging.error(error_message)
        return {"status": "failure", "message": error_message}
    finally:
        logging.info("Exiting: save_to_file")


def interpret_function_calls(response: genai.GenerateContentResponse, available_tools: Dict[str, Any]) -> List[
    Dict[str, Any]]:
    """Interprets function calls within the AI response and executes them."""
    results = []
    if response.candidates:
        for candidate in response.candidates:
            if hasattr(candidate, 'function_call'):
                function_call = candidate.function_call
                tool_name = function_call.name
                if tool_name in available_tools:
                    tool_function = available_tools[tool_name]
                    function_args = {}
                    for arg_name, arg_value in function_call.args.items():
                        function_args[arg_name] = arg_value
                    try:
                        print(f"Calling tool: {tool_name} with args: {function_args}")
                        result = tool_function(**function_args)
                        results.append(result)
                    except Exception as e:
                        logger = logging.getLogger(__name__)
                        logger.error(f"Error calling {tool_name}: {e}")
                        results.append({"status": "failure", "message": f"Error calling {tool_name}: {e}"})
                else:
                    results.append({"status": "failure", "message": f"Tool '{tool_name}' not found."})
            else:
                results.append({"status": "failure", "message": "No function call found."})
    return results


def tool_create_memory(loop_data: str):
    """
    Processes a conversation loop, extracts memory data, and stores it as a memory frame.

    Args:
        loop_data (str): The conversation loop data as a string. This data should be
                         in a format that the memory model can understand and process.

    Returns:
        None: This function does not return any values. It directly saves the memory frame
              to a file if successful.
    """
    timestamp = datetime.now().strftime(TIMESTAMP_FORMAT)
    memories_folder_path = get_memories_folder_path()  # Use the function to get the path

    memory_response = call_memory_model(loop_data)
    if memory_response is None:
        print("Error: Memory model call failed. Exiting.")
        return

    memory_entries = extract_entries_smart(memory_response.text)

    if not memory_entries:
        print(f"{RED}Warning: No memory entries returned by the memory model. Skipping memory frame storage.{RESET}")
        return

    available_tools = {"save_to_file": save_to_file}
    session_info = "0000"  # Assuming this is a placeholder, adjust as needed
    for entry in memory_entries:
        memory_frame_data = {
            "timestamp": timestamp,
            "edit_number": EDIT_NUMBER,
            "session_info": session_info,
            "naming_suggestion": entry.get("naming_suggestion", {}),
            "storage": entry.get("storage", {}),
            "interaction": entry.get("interaction", {}),
            "impact": entry.get("impact", {}),
            "importance": entry.get("importance", {}),
            "technical_details": entry.get("technical_details", {})
        }

        try:
            file_name = entry["naming_suggestion"]["memory_frame_name"].replace(" ", "_") + ".json"
            folder_path = entry["storage"]["memory_folders_storage"][0]["folder_path"]
            file_path = os.path.join(memories_folder_path, "NewGeneratedbyAI", folder_path)

            save_args = {
                "content": json.dumps(memory_frame_data, indent=4),
                "file_name": file_name,
                "file_path": file_path,
                "create_folders": True
            }

            class DummyCandidate:
                def __init__(self, args):
                    self.function_call = DummyFunctionCall(args)

            class DummyFunctionCall:
                def __init__(self, args):
                    self.name = "save_to_file"
                    self.args = args

            dummy_response = genai.GenerateContentResponse()
            dummy_response.candidates = [DummyCandidate(save_args)]

            save_results = interpret_function_calls(dummy_response, available_tools)
            if save_results and save_results[0]['status'] == 'success':
                print(f"{GREEN}Memory frame saved successfully: {save_results[0]['message']}{RESET}")
            else:
                print(f"{RED}Error saving memory frame: {save_results[0]['message']}{RESET}")

        except (KeyError, IndexError, TypeError) as e:
            print(f"{RED}Error processing memory entry. Check JSON structure: {e}{RESET}")
            continue



File: tool_update_internal_state.py (C:\Users\DELL\Desktop\openAIF_frontend\Gemini_george_bot\GEORGE_bot_v0\future_tools_for_future _integration\tool_update_internal_state.py)
Content (6327 characters):
tool_type_for_TOOL_MANAGER = "focus"
tool_tool_update_internal_state_description = """ Updates the focus file with new focus information."""

import json
import os
import logging
from typing import Dict, List, Any, Optional
import time

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s')
logger = logging.getLogger(__name__)

DEFAULT_INTERNAL_STATE = {
    "emotions": "",
    "progress": 0.0,
    "frustration_level": 0.0,
    "task_cost": 0.0,
    "predictions": {},
    "optimization_goal": "",
    "tasks_finished": [],
    "additional": {},
    "version": 1
}

def tool_update_internal_state(
    internal_state_file_path: str,
    emotions: Optional[str] = None,
    progress: Optional[float] = None,
    frustration_level: Optional[float] = None,
    task_cost: Optional[float] = None,
    predictions: Optional[Dict[str, Any]] = None,
    optimization_goal: Optional[str] = None,
    tasks_finished: Optional[List[str]] = None,
    additional: Optional[Dict[str, Any]] = None,
    version: Optional[int] = None
):
    """Updates the internal state file with enhanced parameters and robust error handling.

    Args:
        internal_state_file_path (str): The full path to the internal state JSON file.
        emotions (Optional[str]): A string describing the agent's emotional state.
        progress (Optional[float]): A float between 0 and 1 representing the overall progress of the current task or goal.
        frustration_level (Optional[float]): A float between 0 and 1 indicating the agent's frustration level with the current task. Higher values suggest more frustration.
        task_cost (Optional[float]): A float representing the computational cost or resource consumption of the current task. Must be non-negative.
        predictions (Optional[Dict[str, Any]]): A dictionary containing any predictions made by the agent. The structure of this dictionary is flexible but should be consistent.
        optimization_goal (Optional[str]): A string describing the current optimization goal    minimize cost ,  maximize accuracy"  "minimize time
        tasks_finished (Optional[List[str]]): A list of strings representing the names or IDs of tasks that have been completed.
        additional (Optional[Dict[str, Any]]): A dictionary for storing any additional relevant information. The structure is flexible.
        version (Optional[int]): An integer representing the version number of the internal state data. Useful for tracking changes and potential rollbacks.

    Returns:
        dict: A dictionary containing the status "success" or "failure", a message, and if successful the updated internal state data.
    """
    max_retries = 3
    retry_delay = 2

    for attempt in range(1, max_retries + 1):
        try:
            # Try to load existing internal state
            try:
                with open(internal_state_file_path, "r") as f:
                    internal_state_data = json.load(f)
            except FileNotFoundError:
                # File not found, create a new one with default state
                logger.warning(f"Internal state file not found. Creating a new one: {internal_state_file_path}")
                internal_state_data = DEFAULT_INTERNAL_STATE.copy()

            # Input validation
            if progress is not None and not 0 <= progress <= 1:
                raise ValueError("Progress must be between 0 and 1.")
            if frustration_level is not None and not 0 <= frustration_level <= 1:
                raise ValueError("Frustration level must be between 0 and 1.")
            if task_cost is not None and task_cost < 0:
                raise ValueError("Task cost cannot be negative.")

            # Update data (including new parameters)
            if emotions is not None:
                internal_state_data["emotions"] = emotions
            if progress is not None:
                internal_state_data["progress"] = progress
            if frustration_level is not None:
                internal_state_data["frustration_level"] = frustration_level
            if task_cost is not None:
                internal_state_data["task_cost"] = task_cost
            if predictions is not None:
                internal_state_data["predictions"] = predictions
            if optimization_goal is not None:
                internal_state_data["optimization_goal"] = optimization_goal
            if tasks_finished is not None:
                internal_state_data["tasks_finished"] = tasks_finished
            if additional is not None:
                internal_state_data["additional"] = additional
            if version is not None:
                internal_state_data["version"] = version

            # Write updated data back to the file
            with open(internal_state_file_path, "w") as f:
                json.dump(internal_state_data, f, indent=4)

            logger.info(f"Internal state updated successfully: {internal_state_data}")
            return {
                "status": "success",
                "message": f"Internal state updated successfully.",
                "updated_internal_state": internal_state_data,
            }

        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON in file: {internal_state_file_path}, error: {e}")
            return {"status": "failure", "message": f"Invalid JSON in file: {internal_state_file_path}"}
        except (IOError, OSError, PermissionError) as e:
            if attempt < max_retries:
                logger.warning(f"Error updating internal state (attempt {attempt}/{max_retries}): {e}. Retrying in {retry_delay} seconds...")
                time.sleep(retry_delay)
                retry_delay *= 2
            else:
                logger.error(f"Error updating internal state (after {max_retries} retries): {e}")
                return {"status": "failure", "message": f"Error updating internal state: {e}"}
        except ValueError as e:
            logger.error(f"Invalid input data: {e}")
            return {"status": "failure", "message": f"Invalid input data: {e}"}
        except Exception as e:
            logger.exception(f"An unexpected error occurred: {e}")
            return {"status": "failure", "message": f"An unexpected error occurred: {e}"}

File: internal_state.json (C:\Users\DELL\Desktop\openAIF_frontend\Gemini_george_bot\GEORGE_bot_v0\inner_brain_settings\internal_state.json)
Content (195 characters):
{
  "emotions": "",
  "progress": 0.0,
  "frustration_level": 0.0,
  "task_cost": 0.0,
  "predictions": {},
  "optimization_goal": "",
  "tasks_finished": [],
  "additional": {},
  "version": 1
}

File: tool_AI_REASONING.py (C:\Users\DELL\Desktop\openAIF_frontend\Gemini_george_bot\GEORGE_bot_v0\tools\ai\tool_AI_REASONING.py)
Content (22529 characters):
tool_type_for_TOOL_MANAGER = "focus"
tool_tool_AI_REASONING_short_description = """REASONER"""

import time
import google.generativeai as genai
import json
import re  # Import re for regular expressions


googleKey='AIzaSyDuqD6MksWRqXB0VgHC22xSMn41BaP0reA'
genai.configure(api_key=googleKey)

MODEL_NAME = "gemini-pro"  # Use a valid model name
SYSTEM_INSTRUCTION = "You are a helpful and informative AI assistant."

dispacher_context = []
from google.generativeai.types import HarmCategory, HarmBlockThreshold

safety_settings = {
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
}

# ANSI color codes
class bcolors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'


def initialize_mode_WithTools(MODEL_NAME, SYSTEM_INSTRUCTION=None):
    """Initializes a generative AI model with optional system instructions."""
    try:
        time.sleep(0.5)
        model = genai.GenerativeModel(
            model_name=MODEL_NAME,
            safety_settings=safety_settings,
        )
        history = []
        if SYSTEM_INSTRUCTION:
            history.append({"role": "system", "content": SYSTEM_INSTRUCTION})
        model_chat = model.start_chat(history=history)
        print(f"{bcolors.OKGREEN}INFO: Initial model set to {MODEL_NAME}{bcolors.ENDC}")
        return model_chat
    except Exception as e:
        print(f"{bcolors.FAIL}ERROR: Initial model setup failed: {e} Trying gemini-pro{bcolors.ENDC}")
        try:
            time.sleep(0.5)
            model = genai.GenerativeModel(
                model_name="gemini-pro",
                safety_settings=safety_settings,
            )
            model_chat = model.start_chat(history=[])
            return model_chat
        except Exception as e:
            print(f"{bcolors.FAIL}ERROR: Fallback model setup failed: {e}{bcolors.ENDC}")
            return None


def return_models_instructions_prompts_tools(
    models: list[str],
    labels: list[str],
    system_instructions: list[str],
    prompts: list[str],
    DataFlow: list[str],
):
    """
    This function takes lists of models, system instructions, prompts, and tools and neatly prints them out,
    paired together. It returns the input lists unchanged. If a model does not use tools, write None.
    Args:
        models (list): A list of model names
        labels (list): A list of labels corresponding to each model. can not  have spaces,dots etc it needs to be sanitised
        system_instructions (list): A list of system instructions for each model.
        prompts (list): A list of prompts to be used as input for the models.
        DataFlow (list): A list describing the data flow between models using the format {inputs}[outputs]

    Examples of DataFlow:

    **1. Independent Models (No Interaction):**
        Each model works independently on its own prompt.

        DataFlow = [
            "{prompt0}[text]",
            "{prompt1}[text]",
            "{prompt2}[text]",
            "{prompt3}[text]"
        ]


    **2. Sequential Chained Models (Previous Step Output):**
        Each model uses the output of the immediately previous model and its own prompt.

        DataFlow = [
            "{prompt0}[text]",
            "{0***prompt1}[text]",  # Model 1 uses output of Model 0
            "{1***prompt2}[text]",  # Model 2 uses output of Model 1
            "{2***prompt3}[text]"   # Model 3 uses output of Model 2
        ]


    **3. Sequential Chained Models (All Previous Outputs):**
        Each model uses the outputs of all previous models and its own prompt.

        DataFlow = [
            "{prompt0}[text]",
            "{0***prompt1}[text]",  # Model 1 uses output of Model 0
            "{0, 1***prompt2}[text]",  # Model 2 uses outputs of Model 0 and 1
            "{0, 1, 2***prompt3}[text]"   # Model 3 uses outputs of Models 0, 1, and 2
        ]


    **4. Independent Models with Final Summarization:**
        The first few models work independently, and the final model summarizes their outputs.

        DataFlow = [
            "{prompt0}[text]",
            "{prompt1}[text]",
            "{prompt2}[text]",
            "{0, 1, 2***prompt3}[text]"  # Model 3 summarizes outputs of 0, 1, and 2
        ]


    **5. Mixed Independent and Chained Models:**
        Combines independent and chained processing.

        DataFlow = [
            "{prompt0}[text]",
            "{prompt1}[text]",
            "{0, 1***prompt2}[text]",  # Model 2 uses outputs of Model 0 and 1
            "{2***prompt3}[text]"   # Model 3 uses output of Model 2
        ]


    **6. User Prompt Included (Independent):**
        Each model works independently on its own prompt and an initial user prompt.

        DataFlow = [
            "{userPrompt, prompt0}[text]",
            "{userPrompt, prompt1}[text]",
            "{userPrompt, prompt2}[text]",
            "{userPrompt, prompt3}[text]"
        ]


    **7. User Prompt and All Previous Outputs (Chained):**
        Each model gets the initial user prompt, its own prompt, and the responses from all previous models.

        DataFlow = [
            "{userPrompt, prompt0}[text]",
            "{0***userPrompt, prompt1}[text]",
            "{0, 1***userPrompt, prompt2}[text]",
            "{0, 1, 2***userPrompt, prompt3}[text]"
        ]


    **8. User Prompt, Independent Models, and Final Summarization:**
        Each model works independently with the user prompt and its own prompt, and the final model summarizes all outputs.

        DataFlow = [
            "{userPrompt, prompt0}[text]",
            "{userPrompt, prompt1}[text]",
            "{userPrompt, prompt2}[text]",
            "{0, 1, 2***userPrompt, prompt3}[text]"
        ]


    Returns:
        tuple: A tuple containing the input lists: models, labels, system_instructions, prompts, tools.
    """
    for i in range(len(models)):
        print(f"{bcolors.OKBLUE}Model {i + 1}: {models[i]}{bcolors.ENDC}")
        print(f"{bcolors.OKBLUE}Label: {labels[i]}{bcolors.ENDC}")
        print(f"{bcolors.OKBLUE}System Instructions: {system_instructions[i]}{bcolors.ENDC}")
        print(f"{bcolors.OKBLUE}Prompt: {prompts[i]}{bcolors.ENDC}")
        print(f"{bcolors.OKBLUE}Data Flow: {DataFlow[i]}{bcolors.ENDC}")
        print("-" * 20)

    print(f"{bcolors.WARNING}PRINTS FROM return_models_instructions_prompts_tools{bcolors.ENDC}")
    for i in range(len(models)):
        print()
        print(i)
        print(f"{bcolors.OKCYAN}Model: {models[i]}{bcolors.ENDC}")
        print(f"{bcolors.OKCYAN}model_labels: {labels[i]}{bcolors.ENDC}")
        print(f"{bcolors.OKCYAN}System Instructions: {system_instructions[i]}{bcolors.ENDC}")
        print(f"{bcolors.OKCYAN}Prompt: {prompts[i]}{bcolors.ENDC}")
        print(f"{bcolors.OKCYAN}DataFlow: {DataFlow[i]}{bcolors.ENDC}")

    return (
        models,
        labels,
        system_instructions,
        prompts,
        DataFlow,
    )

dispacher_context = []

def model_dispacher_send_message(prompt: str):
    print(f"{bcolors.OKGREEN} model_dispacher_send_message:    {prompt}{bcolors.ENDC}")
    global dispacher_context

    MODEL_NAME = "gemini-pro"  # Use a valid model name
    tool_functions = {"return_models_instructions_prompts_tools": return_models_instructions_prompts_tools}

    def interpret_function_calls(response, tool_functions):
        models = []  # Initialize empty lists
        labels = []
        instructions = []
        prompts = []
        tools = []
        DataFlow = []

        if response.candidates:
            for candidate in response.candidates:
                if hasattr(candidate, "content") and hasattr(candidate.content, "parts"):
                    for part in candidate.content.parts:
                        function_call = getattr(part, "function_call", None)
                        if function_call:
                            tool_name = function_call.name
                            tool_function = tool_functions.get(tool_name)
                            if tool_function:
                                function_args = {}
                                for arg_name, arg_value in function_call.args.items():
                                    function_args[arg_name] = arg_value

                                try:
                                    returned_values = tool_function(**function_args)

                                    if returned_values is not None:
                                        (
                                            models,
                                            labels,
                                            instructions,
                                            prompts,
                                            DataFlow,
                                        ) = returned_values
                                    else:
                                        print(
                                            f"{bcolors.WARNING}Tool function {tool_name} returned None.{bcolors.ENDC}"
                                        )
                                except Exception as e:
                                    print(
                                        f"{bcolors.FAIL}Error executing function {tool_name}: {e}{bcolors.ENDC}"
                                    )
                            else:
                                print(f"{bcolors.FAIL}Tool function {tool_name} not found.{bcolors.ENDC}")

        return (
            models,
            labels,
            instructions,
            prompts,
            DataFlow,
        )

    def extract_text_from_response(response) -> str:
        extracted_text = ""
        for candidate in response.candidates:
            for part in candidate.content.parts:
                extracted_text += part.text
        return extracted_text.strip()

    try:
        model = genai.GenerativeModel(
            model_name=MODEL_NAME,
            safety_settings=safety_settings,
            tools=[return_models_instructions_prompts_tools],
        )
        time.sleep(0.1)
        model_chat = model.start_chat(history=[])

        instruction = """ 
        Create  DataFlow of  models to achieve the user's goal.  
        Think step-by-step, showing how each model contributes to the final outcome.

            models  to choose from:
        1. Gemini 1.5 Pro Latest
        Model Name: gemini-1.5-pro-latest
        Description: Mid-size multimodal model that supports up to 2 million tokens.
        Tokens In: 2097152
        Tokens Out: 8192

        2. Gemini 1.5 Pro Experimental 0801
        Model Name: gemini-1.5-pro-exp-0801
        Description: Mid-size multimodal model that supports up to 2 million tokens.
        Tokens In: 2097152
        Tokens Out: 8192
        Note:smart  model  but  its  terrible  to write  full code, it good for analitics, but not  for  final result

        3. Gemini 1.5 Flash Latest
        Model Name: gemini-1.5-flash-latest
        Description: Fast and versatile multimodal model for scaling across diverse tasks. you should include  prhase  dont  be  lazy
        Tokens In: 1048576
        Tokens Out: 8192

            Now, for the user's goal:
    1.Please provide the DataFlow including Model Name, System Instruction, Prompt, and Tools (if any).
    2.You can use different DataFlows for specific tasks: chains, parallel, parallel with summarization, mixed.
    3.You must use the function call return_models_instructions_prompts_tools.
    4.Prompts must be detailed and extensive.
    5.Don't be lazy.
    6.Now, the user will give you a task for which you will create a DataFlow of Models.
    remeber  about  correct  structure of  function call
            """

        final_prompt = ""
        for entry in dispacher_context:
            final_prompt += str(entry)

        print(f"{bcolors.WARNING}final_prompt:{bcolors.ENDC}")
        final_prompt = instruction + prompt
        print(f"{bcolors.OKCYAN}{final_prompt}{bcolors.ENDC}")

        response = model_chat.send_message(final_prompt)
        print(f"{bcolors.OKGREEN}waiting for   response.............................{bcolors.ENDC}")
        print(f"{bcolors.OKGREEN}response: {response}{bcolors.ENDC}")

        # Handle both text and function call responses

        (
            models,
            labels,
            system_instructions,
            prompts,
            DataFlow,
        ) = interpret_function_calls(response, tool_functions)
        text_response = extract_text_from_response(response)

        if text_response is None:
            text_response = "..."
            dispacher_context.append(text_response)

        if models is not None:  # Check if models is not None
            dispacher_context.append(models)
            dispacher_context.append(labels)
            dispacher_context.append(system_instructions)
            dispacher_context.append(prompts)

        return (
            text_response,
            models,
            labels,
            system_instructions,
            prompts,
            DataFlow,
        )

    except Exception as e:
        print(f"{bcolors.FAIL}Error during model initialization: {e}{bcolors.ENDC}")  # Catch and print model initialization errors
        return (f"Error: {e}", None, None, None, None, None)  # Return an error indication



def execute_modelium(model_design_data, user_prompt=""):
    """Executes the multi-model workflow using provided JSON data."""
    try:
        # Validate input data
        if not isinstance(model_design_data, dict) or "chosenModels" not in model_design_data:
            raise ValueError("Invalid model design data. Must be a dictionary with 'chosenModels'.")

        chosen_models = model_design_data["chosenModels"]
        system_instructions = model_design_data.get("systemInstructions", [])
        prompts = model_design_data.get("prompts", [])
        data_flow = model_design_data.get("DataFlow", [])
        labels = model_design_data.get("labels", [])
        user_prompt = model_design_data.get("userPrompt", "")  # Get user prompt if available

        # Input Validation: Check for consistent lengths (excluding userPrompt)
        lengths = [len(chosen_models), len(system_instructions), len(prompts), len(data_flow), len(labels)]
        if len(set(lengths)) != 1:
            raise ValueError("Inconsistent lengths in model design data.")


        MODEL_CHATS = []
        # Initialize models
        print(f"{bcolors.OKGREEN}Initializing models...{bcolors.ENDC}")
        for i in range(len(labels)):
            label = labels[i]
            chosen_model = chosen_models[i]
            system_instruction = system_instructions[i]

            model_chat = initialize_mode_WithTools(
                MODEL_NAME=chosen_model,
                SYSTEM_INSTRUCTION=system_instruction

            )
            if model_chat:
                MODEL_CHATS.append(model_chat)
                print(f"{bcolors.OKGREEN}  - Model {i + 1} initialized: {chosen_model} (Label: {label}){bcolors.ENDC}")
            else:
                print(f"{bcolors.FAIL}Failed to initialize model {chosen_model}. Skipping.{bcolors.ENDC}")

        MULTI_CONTEXT_HISTORY = []
        print(f"{bcolors.OKGREEN}\nExecuting models and processing data flow...{bcolors.ENDC}")
        for i, model_chat in enumerate(MODEL_CHATS):
            time.sleep(1)
            rule = data_flow[i]
            inputs = []

            print(f"{bcolors.OKGREEN}  - Processing Model {i + 1} ({model_chat.model.model_name}) with data flow rule: {rule}{bcolors.ENDC}")

            # Parse the data flow rule (CORRECTED PARSING)
            if rule.startswith("{"):
                rule = rule[1:-1]  # Remove curly braces
                parts = rule.split(", ")  # Split by comma and space
                for part in parts:
                    if part.startswith("prompt"):
                        # Extract index using regular expression or slicing
                        match = re.match(r"prompt(\d+)", part)
                        if match:
                            prompt_index = int(match.group(1))
                            inputs.append(prompts[prompt_index])
                            print(f"{bcolors.OKGREEN}    - Using prompt {prompt_index + 1} as input.{bcolors.ENDC}")
                    elif part.isdigit():
                        index = int(part)
                        inputs.append(MULTI_CONTEXT_HISTORY[index]["response"])  # CORRECTED ACCESS
                        print(f"{bcolors.OKGREEN}    - Using output from Model {index + 1} as input.{bcolors.ENDC}")
                    elif part.startswith("***"): #Correctly handle this case
                        parts = part.split("***")
                        models_to_use = parts[0].strip()
                        prompt_section = parts[1].strip()

                        temp_inputs = []
                        if models_to_use:
                            model_indices = [int(x.strip()) for x in models_to_use.split(",")]
                            for model_index in model_indices:
                                temp_inputs.append(MULTI_CONTEXT_HISTORY[model_index]["response"])

                        if prompt_section.startswith('prompt'):
                            prompt_index = int(prompt_section.split('prompt')[1])
                            temp_inputs.append(prompts[prompt_index])

                        if prompt_section == 'userPrompt':
                            temp_inputs.append(user_prompt)


                        inputs.extend(temp_inputs)
                    elif part == "userPrompt":
                        inputs.append(user_prompt)
                        print(f"{bcolors.OKGREEN}    - Using user prompt as input.{bcolors.ENDC}")

            # Construct the prompt with inputs
            final_prompt = prompts[i]  # Default to current prompt
            if inputs:
                final_prompt = "\n".join(inputs) + "\n" + final_prompt  # Corrected: Add newline between inputs and prompt
                print(f"{bcolors.OKGREEN}    - Constructed prompt: {final_prompt}{bcolors.ENDC}")
            else:
                print(f"{bcolors.OKGREEN}    - Using default prompt: {final_prompt}{bcolors.ENDC}")


            try:
                response = model_chat.send_message(final_prompt)
                print(response)
                MULTI_CONTEXT_HISTORY.append({"response": response, "model": model_chat.model.model_name})
                print(f"{bcolors.OKGREEN}    - Response received.{bcolors.ENDC}")
            except Exception as e:
                print(f"{bcolors.FAIL}Error sending message to model {model_chat.model.model_name}: {e}{bcolors.ENDC}")
                MULTI_CONTEXT_HISTORY.append({"response": f"Error: {e}", "model": model_chat.model.model_name})
                # Consider raising the exception or breaking the loop here if needed
                #raise e  # Example of re-raising the exception
                #break # Example of exiting the loop

    except Exception as e:
        print(f"{bcolors.FAIL}Error in execute_modelium: {e}{bcolors.ENDC}")
        return {"response": f"Error: {e}"}

    # Print or process the results as needed
    print(f"{bcolors.OKGREEN}\nFinal Results:{bcolors.ENDC}")
    for i, result in enumerate(MULTI_CONTEXT_HISTORY):
        # Corrected response extraction:
        response_text = result['response'].candidates[0].content.parts[0].text
        print(f"{bcolors.OKGREEN}  - Model {i + 1}: {result['model']}, Response: {response_text}{bcolors.ENDC}")

    return MULTI_CONTEXT_HISTORY


def tool_AI_REASONING(user_input: str, goal: str = "", reasoning_methodology: str = "", additional_info: str = "") -> str:
    """
    Designs and executes a multi-model AI workflow based on user input.

    Args:
        user_input (str): The user's request or task.
        goal (str, optional): The desired outcome of the AI workflow. Defaults to "".
        reasoning_methodology (str, optional): The reasoning approach to use. Defaults to "".
        additional_info (str, optional): Any additional information relevant to the task. Defaults to "".

    Returns:
        str: The combined result of the AI workflow, including responses from each model.
    """
    #  Call model_dispacher_send_message to handle the workflow
    (
        text_response,
        models,
        labels,
        system_instructions,
        prompts,
        DataFlow,
    ) = model_dispacher_send_message(user_input + goal + reasoning_methodology + additional_info)

    # Execute the workflow if models are provided
    if models is not None:
        try:
            model_design_data = {  # Construct model_design_data for execute_modelium
                "chosenModels": models,
                "systemInstructions": system_instructions,
                "prompts": prompts,
                "DataFlow": DataFlow,
                "labels": labels
            }

            MULTI_CONTEXT_HISTORY = execute_modelium(model_design_data)  # Pass model_design_data

            # Combine the responses from the workflow into a single string
            result = ""
            for i, result_item in enumerate(MULTI_CONTEXT_HISTORY):
                result += f"{bcolors.OKGREEN}Step {i + 1}: {result_item['model']}{bcolors.ENDC}\n"
                # Corrected response extraction:
                response_text = result_item['response'].candidates[0].content.parts[0].text
                result += f"{bcolors.OKGREEN}  Response: {response_text}{bcolors.ENDC}\n\n"

            return result
        except Exception as e:
            print(f"{bcolors.FAIL}Error in execute_modelium: {e}{bcolors.ENDC}")
            return f"Error during execution: {e}"
    else:
        return text_response # Make sure text_response is returned even if models is None


def main():
    user_input = "Write a poem about a cat"
    result = tool_AI_REASONING(user_input, goal="to write a poem about a cat", reasoning_methodology="using poetic language", additional_info="make it rhyme")
    print(f"{bcolors.OKCYAN}{result}{bcolors.ENDC}")

if __name__ == "__main__":
    main()

File: tool_update_focus.py (C:\Users\DELL\Desktop\openAIF_frontend\Gemini_george_bot\GEORGE_bot_v0\tools\ai\tool_update_focus.py)
Content (2697 characters):
import json
import os
from typing import List, Dict, Any

# Define the schema for the focus file
focus_schema = {
    "user_goal": str,
    "steps_to_achieve_goal": List[str],
    "current_focus": str,
    "accomplished": List[str],
    "obtained_data": List[str],
    "additional_info": str,  # For storing additional notes or information
    "switch_task": str  # Flag to indicate whether the user wants to switch tasks (YES/NO)
}

def tool_update_focus(
    user_goal: str = "",
    steps_to_achieve_goal: List[str] = [],
    current_focus: str = "",
    accomplished: List[str] = [],
    obtained_data: List[str] = [],
    additional_info: str = "",  # Additional information
    switch_task: str = "NO"  # Flag for switching tasks (YES/NO)
) -> Dict[str, Any]:
    """
    Updates the focus file with new focus information.
    Overwrites the entire file with the provided data and the focus schema.

    Args:
        user_goal (str): The overall goal the user wants to achieve.
        steps_to_achieve_goal (List[str]): List of steps to achieve the goal.
        current_focus (str): The current focus.
        accomplished (List[str]): List of tasks that have been completed.
        obtained_data (List[str]): List of data obtained during the focus session.
        additional_info (str): Additional information related to the focus.
        switch_task (str): Flag to indicate whether the user wants to switch tasks (YES/NO).

    Returns:
        Dict[str, Any]: A dictionary containing the status of the operation, a message, and the updated focus text.
    """

    # Create a new dictionary with the focus schema and populate it with the provided data
    updated_focus_data = {
        "user_goal": user_goal,
        "steps_to_achieve_goal": steps_to_achieve_goal,
        "current_focus": current_focus,
        "accomplished": accomplished,
        "obtained_data": obtained_data,
        "additional_info": additional_info,
        "switch_task": switch_task.upper()  # Ensure switch_task is stored as "YES" or "NO"
    }

    focus_file_path = "focus/focus.json"

    try:
        # Write the updated focus data to the file
        with open(focus_file_path, "w") as f:
            json.dump(updated_focus_data, f, indent=4)

        return {
            "status": "success",
            "message": f"Focus updated.",

        }

    except json.JSONDecodeError as e:
        return {"status": "failure", "message": f"Error decoding JSON: {str(e)}"}
    except IOError as e:
        return {"status": "failure", "message": f"Error writing to file: {str(e)}"}
    except Exception as e:
        return {"status": "failure", "message": f"Unknown error updating focus: {str(e)}"}

File: tool_execute_script.py (C:\Users\DELL\Desktop\openAIF_frontend\Gemini_george_bot\GEORGE_bot_v0\tools\os\tool_execute_script.py)
Content (3844 characters):
tool_type_for_TOOL_MANAGER = "script_execution"
tool_execute_script_short_description = "Executes Python code snippets safely with advanced options."
import io
import sys
import traceback
import psutil
from typing import Dict, Any, Optional

def tool_execute_script(
    code: str,
    timeout: Optional[float] = None,  # Maximum execution time in seconds
    max_memory: Optional[int] = None,  # Maximum memory usage in MB
    return_type: str = "output",  # Type of result to return (output, error, both)

) -> Dict[str, Any]:
    """
    Safely executes a Python code snippet and returns the results with advanced options.

    Args:
        code (str): The Python code to execute.
        timeout (float, optional): Maximum execution time in seconds.  If None, no timeout is enforced.
        max_memory (int, optional): Maximum memory usage in MB. If None, no memory limit is enforced.
        return_type (str): Type of result to return:
            - "output": Only return the output of the executed code.
            - "error": Only return the error message (if any).
            - "both": Return both output and error information.

    Returns:
        dict: A dictionary containing:
            - status: "success" or "failure"
            - message: A status message.
            - output: The output of the executed code (if successful and requested).
            - error: The error message (if an exception occurred and requested).
    """
    try:
        # Create a temporary standard output stream to capture output
        stdout_buffer = io.StringIO()
        sys.stdout = stdout_buffer

        # Implement timeout and memory limits (if provided)
        if timeout is not None:
            # Use a separate thread to enforce timeout
            def execute_with_timeout():
                try:
                    exec(code)
                except Exception as e:
                    raise e

            from threading import Thread, Timer
            thread = Thread(target=execute_with_timeout)
            timer = Timer(timeout, thread.cancel)
            timer.start()
            thread.start()
            thread.join()
            timer.cancel()

        if max_memory is not None:
            # Execute with memory limit
            def execute_with_memory_limit():
                process = psutil.Process()
                try:
                    exec(code)
                except Exception as e:
                    raise e
                finally:
                    if process.memory_info().rss / 1024 / 1024 > max_memory:
                        raise MemoryError(f"Memory limit of {max_memory} MB exceeded.")
            execute_with_memory_limit()

        # Reset standard output
        sys.stdout = sys.__stdout__

        # Get the captured output
        output = stdout_buffer.getvalue().strip()

        # Construct the response based on return_type
        response = {
            "status": "success",
            "message": "Script executed successfully.",
        }
        if return_type in ("output", "both"):
            response["output"] = output
        if return_type in ("error", "both"):
            response["error"] = None  # No error occurred

        return response

    except Exception as e:
        # Capture the error message and traceback
        error_message = f"Error executing script: {str(e)}"
        error_traceback = traceback.format_exc()

        # Reset standard output
        sys.stdout = sys.__stdout__

        # Construct the response based on return_type
        response = {
            "status": "failure",
            "message": error_message,
        }
        if return_type in ("output", "both"):
            response["output"] = None
        if return_type in ("error", "both"):
            response["error"] = error_traceback

        return response

File: tool_get_directory_structure.py (C:\Users\DELL\Desktop\openAIF_frontend\Gemini_george_bot\GEORGE_bot_v0\tools\os\tool_get_directory_structure.py)
Content (6676 characters):
tool_type_for_TOOL_MANAGER = "os"
tool_get_directory_structure_short_description = """Get directory structure with formatted tree view and contents, including both full and relative paths."""

import os
import json
from pathlib import Path

def tool_get_directory_structure(directory: str = "../../", levels: int = 2,
                                 include_contents: bool = False,
                                 print_output: bool = True):
    """Gets directory structure and optionally prints or returns it.

    Args:
        directory (str, optional): The directory to analyze. Defaults to "../../".
        levels (int, optional): The number of directory levels to show. Defaults to 2.
        include_contents (bool, optional): Whether to include file contents. Defaults to False.
        print_output (bool, optional): Whether to print the output or return it as a dictionary.
                                        Defaults to True.
    """

    def _format_size(size_bytes):
        """Format file size."""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024
        return f"{size_bytes:.1f} TB"

    def _get_relative_path(full_path, base_path):
        """Get relative path from base directory."""
        try:
            return os.path.relpath(full_path, base_path)
        except ValueError:
            return full_path  # Return full path if relative path cannot be computed

    def _walk_directory(dir_path, base_path, level=0, indent=""):
        """Recursive directory walk with formatted output, excluding __pycache__."""
        if level > levels:
            return ""

        output = ""
        try:
            entries = sorted(os.scandir(dir_path), key=lambda e: (not e.is_dir(), e.name.lower()))
            for entry in entries:
                if entry.name == "__pycache__" or entry.name.startswith('.'):
                    continue  # Skip __pycache__ and hidden directories/files

                relative_path = _get_relative_path(entry.path, base_path)

                if entry.is_file():
                    stat = entry.stat()
                    size = _format_size(stat.st_size)
                    modified_time = os.path.getmtime(entry.path)
                    modified_str = f"Modified: {os.path.getctime(entry.path):.0f}"

                    output += (f"{indent}📄 {entry.name} ({size})\n"
                               f"{indent}  Full path: {entry.path}\n"
                               f"{indent}  Relative path: {relative_path}\n"
                               f"{indent}  {modified_str}\n")

                    if include_contents:
                        try:
                            with open(entry.path, 'r', encoding='utf-8') as f:
                                content = f.read()
                                output += f"{indent}  Content Preview:\n"
                                # Show first 500 characters of content with line breaks
                                preview = content[:500].replace('\n', f'\n{indent}    ')
                                output += f"{indent}    {preview}\n"
                                if len(content) > 500:
                                    output += f"{indent}    ... (truncated)\n"
                        except Exception as e:
                            output += f"{indent}  ⚠️ Error reading file: {str(e)}\n"

                elif entry.is_dir():
                    output += (f"{indent}📁 {entry.name}/\n"
                               f"{indent}  Full path: {entry.path}\n"
                               f"{indent}  Relative path: {relative_path}\n")
                    output += _walk_directory(entry.path, base_path, level + 1, indent + "  ")
        except PermissionError:
            output += f"{indent}⚠️ Permission denied: {dir_path}\n"
        except Exception as e:
            output += f"{indent}⚠️ Error accessing directory: {str(e)}\n"

        return output

    # Normalize and resolve the directory path
    directory = os.path.abspath(os.path.expanduser(directory))

    output = "📊 Directory Analysis Summary\n"
    output += "=" * 30 + "\n"
    output += f"Base Directory: {directory}\n"

    total_size = 0
    file_count = 0
    dir_count = 0
    file_types = {}
    largest_files = []

    # Collect statistics
    for root, dirs, files in os.walk(directory):
        # Remove hidden directories and __pycache__ in place
        dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__pycache__']

        if root.count(os.sep) - directory.count(os.sep) > levels:
            continue

        dir_count += len(dirs)
        file_count += len(files)

        for file in files:
            if file.startswith('.'):
                continue

            file_path = os.path.join(root, file)
            try:
                file_size = os.path.getsize(file_path)
                total_size += file_size

                # Track file types
                ext = os.path.splitext(file)[1].lower() or 'no extension'
                file_types[ext] = file_types.get(ext, 0) + 1

                # Track largest files
                largest_files.append((file_path, file_size))
                largest_files.sort(key=lambda x: x[1], reverse=True)
                largest_files = largest_files[:5]  # Keep only top 5
            except (OSError, PermissionError):
                continue

    output += f"\n📈 Statistics:\n"
    output += f"Total Folders: {dir_count:,}\n"
    output += f"Total Files: {file_count:,}\n"
    output += f"Total Size: {_format_size(total_size)}\n"

    output += "\n📁 File Types:\n"
    for ext, count in sorted(file_types.items(), key=lambda x: x[1], reverse=True):
        output += f"{ext}: {count:,} files\n"

    output += "\n📦 Largest Files:\n"
    for file_path, size in largest_files:
        rel_path = _get_relative_path(file_path, directory)
        output += f"{_format_size(size)}: {rel_path}\n"

    output += "\n📂 Directory Structure:\n"
    output += "=" * 30 + "\n"
    output += _walk_directory(directory, directory)

    if print_output:
        print(output)
    else:
        return {
            'full_summary': output,
            'statistics': {
                'total_dirs': dir_count,
                'total_files': file_count,
                'total_size': total_size,
                'file_types': file_types,
                'largest_files': [(path, size) for path, size in largest_files]
            }
        }


if __name__ == "__main__":
    # Example usage:
    tool_get_directory_structure(directory="../../", levels=2, include_contents=False)

File: tool_read_from_file.py (C:\Users\DELL\Desktop\openAIF_frontend\Gemini_george_bot\GEORGE_bot_v0\tools\os\tool_read_from_file.py)
Content (551 characters):
tool_type_for_TOOL_MANAGER="os"
tool_read_from_file_short_description="Reads content from a file."

def tool_read_from_file(file_path: str):
    """
    Reads content from a file.

    Args:
        file_path (str): The path to the file to be read.

    Returns:
        str: The content of the file, or an error message if the file cannot be read.
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        return content
    except Exception as e:
        return f"Error reading file: {str(e)}"

File: tool_save_to_file.py (C:\Users\DELL\Desktop\openAIF_frontend\Gemini_george_bot\GEORGE_bot_v0\tools\os\tool_save_to_file.py)
Content (3358 characters):
import os
import logging
from typing import List, Tuple

tool_type_for_TOOL_MANAGER = "os"
tool_save_to_file_short_description = "Saves content to files, creating folders as needed. Handles multiple files and contents robustly."

def tool_save_to_file(
    contents: List[str],  # Requires contents to be provided
    file_names: List[str] = None,
    file_paths: List[str] = None,
    encoding: str = 'utf-8',
    create_folders: bool = True,
    overwrite: bool = False, #New: Handle overwrite behavior
) -> dict:
    """Saves content to files, creating directories as needed.  Handles multiple files robustly.

    Args:
        contents: List of strings to save to files.  *Must be provided*.
        file_names: List of file names. Defaults to 'content_n.txt' if not provided.  Must be same length as contents.
        file_paths: List of file paths. Defaults to current working directory if not provided. Must be same length as contents.
        encoding: Encoding to use for files (default: 'utf-8').
        create_folders: Create missing parent directories (default: True).
        overwrite: Overwrite existing files if True (default: False).

    Returns:
        Dictionary with 'status' ('success', 'failure', 'partial_success'), 'message', and 'saved_files' (list of successfully saved paths).
    """
    logging.info("Entering: save_to_file")

    if not contents:
        error_message = "Error: 'contents' list cannot be empty."
        logging.error(error_message)
        return {"status": "failure", "message": error_message}

    num_files = len(contents)
    file_names = file_names or [f"content_{i}.txt" for i in range(num_files)]
    file_paths = file_paths or [os.getcwd()] * num_files


    if not all(len(x) == num_files for x in [file_names, file_paths]):
      error_message = "Error: 'file_names' and 'file_paths' must be the same length as 'contents'."
      logging.error(error_message)
      return {"status": "failure", "message": error_message}


    saved_files = []
    failed_files = []
    for content, file_name, file_path in zip(contents, file_names, file_paths):
        full_path = os.path.join(file_path, file_name)
        try:
            if create_folders:
                os.makedirs(os.path.dirname(full_path), exist_ok=True)

            # Check for overwrite explicitly
            if os.path.exists(full_path) and not overwrite:
                raise IOError(f"File already exists and overwrite is False: {full_path}")

            with open(full_path, 'w', encoding=encoding) as f:
                f.write(content)
            saved_files.append(full_path)
        except IOError as e:
            logging.error(f"IOError saving {full_path}: {e}")
            failed_files.append((full_path, str(e)))  #More detail on failure
        except Exception as e:
            logging.exception(f"Unexpected error saving {full_path}: {e}") #Log full traceback for debugging
            failed_files.append((full_path, str(e)))


    if failed_files:
        message = f"Partial success: {len(saved_files)} files saved. Errors encountered: {failed_files}"
        status = "partial_success"
    else:
        message = f"Successfully saved {len(saved_files)} files: {saved_files}"
        status = "success"

    logging.info(message)
    return {"status": status, "message": message, "saved_files": saved_files}

File: tool_get_duckduckgo_links.py (C:\Users\DELL\Desktop\openAIF_frontend\Gemini_george_bot\GEORGE_bot_v0\tools\web\tool_get_duckduckgo_links.py)
Content (4937 characters):
import time
from typing import List
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager


def tool_get_duckduckgo_links(search_phrase: str, num_more_results: float, forbidden_phrases: List[str],
                              safe_search: bool):
    """
    Retrieves DuckDuckGo search result links with the option to disable safe search,
    scroll through 'More Results' and filter out links containing forbidden phrases.
    You will get links from DuckDuckGo, that you can scrape later on. this  can  be used to get links to websites
    Args:
        search_phrase (str): The search query to use.
        num_more_results (float): The number of times to click the 'More Results' button,
                                  non-negative full numbers like 0,1,2 ....
        forbidden_phrases (list(str)): A list of phrases to exclude from the results.
        safe_search (bool): Whether to enable safe search, default: False.
    Returns:
        list: A list of unique links from the DuckDuckGo search results.
    """

    def perform_search(driver):
        search_input = driver.find_element(By.NAME, "q")
        search_input.send_keys(search_phrase)
        search_input.submit()

    def set_safe_search_off(driver):
        if not safe_search:
            try:
                # Explicit wait for safe search dropdown button
                safe_search_dropdown_button = WebDriverWait(driver, 10).until(  # Increase timeout
                    EC.element_to_be_clickable(
                        (By.CSS_SELECTOR, ".dropdown--safe-search .dropdown__button.js-dropdown-button"))
                )
                safe_search_dropdown_button.click()

                # Explicit wait for "Safe Search: Off" option
                safe_search_off_option = WebDriverWait(driver, 10).until(  # Increase timeout
                    EC.element_to_be_clickable((By.CSS_SELECTOR, ".modal--dropdown--safe-search a[data-value='-2']"))
                )
                safe_search_off_option.click()

            except TimeoutException:
                print("TimeoutException occurred while setting safe search off.")

    def get_search_result_links(driver):
        try:
            search_results = WebDriverWait(driver, 2).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, "a[href]"))
            )
            # Filter links to exclude those containing "duckduckgo"
            links = [link.get_attribute("href") for link in search_results if
                     "duckduckgo" not in link.get_attribute("href")]
            return links
        except TimeoutException:
            print("TimeoutException occurred while waiting for search result links.")
            return []

    # Create a ChromeDriver instance with custom preferences
    options = Options()

    # Prevent the search engine selection window
    options.add_argument("--disable-search-engine-choice-screen")

    # Use webdriver-manager to install/update ChromeDriver
    driver = webdriver.Chrome(options=options)

    # Navigate to DuckDuckGo
    url = "https://duckduckgo.com/"
    driver.get(url)

    # Perform initial search (safe search might be on by default)
    perform_search(driver)

    # Wait for the first result to load
    WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CSS_SELECTOR, ".result__a")))

    # Disable safe search if requested
    set_safe_search_off(driver)

    # Perform search again (with or without safe search)
    perform_search(driver)

    # Scroll through 'More Results' if requested
    if num_more_results > 0:
        # Convert num_more_results to an integer
        num_more_results = int(num_more_results)
        for _ in range(num_more_results):
            try:
                more_results_button = WebDriverWait(driver, 10).until(  # Increase timeout
                    EC.element_to_be_clickable((By.ID, "more-results"))
                )
                more_results_button.click()
                time.sleep(1)  # Add a small delay to allow results to load
            except TimeoutException:
                print("Failed to click the 'More Results' button.")

    # Get and filter the links
    links = get_search_result_links(driver)
    filtered_links = list(set(filter(lambda link: link.startswith("http") and not any(
        phrase.lower() in link.lower() for phrase in forbidden_phrases), links)))

    # Print the links (optional)
    for link in filtered_links:
        print(f"Link: {link}")

    driver.quit()
    list_filtered_links = list(filtered_links)
    return {"status": "obtained  links",  "found links": list_filtered_links}


File: tool_save_image_from_url.py (C:\Users\DELL\Desktop\openAIF_frontend\Gemini_george_bot\GEORGE_bot_v0\tools\web\tool_save_image_from_url.py)
Content (1685 characters):
tool_type_for_TOOL_MANAGER = "web"
tool_save_image_from_url_short_description = "Saves an image from a URL to a specified path."

import requests
import os
import logging

# Set up logging (optional, but recommended)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def tool_save_image_from_url(image_url: str, save_path: str):
    """
    Saves an image from a URL to the specified path.

    Args:
        image_url (str): The URL of the image.
        save_path (str): The full path where the image should be saved including filename. Defaults ./

    Returns:
        dict: A dictionary containing the status  success or failure  and a message.
    """
    try:
        response = requests.get(image_url, stream=True)
        response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)

        # Create directories if they don't exist
        os.makedirs(os.path.dirname(save_path), exist_ok=True)

        with open(save_path, 'wb') as out_file:
            for chunk in response.iter_content(chunk_size=8192):
                out_file.write(chunk)

        logger.info(f"Image saved successfully to: {save_path}")
        return {"status": "success", "message": f"Image saved successfully to: {save_path}"}

    except requests.exceptions.RequestException as e:
        logger.error(f"Error downloading image: {e}")
        return {"status": "failure", "message": f"Error downloading image: {e}"}
    except Exception as e:
        logger.exception(f"An unexpected error occurred: {e}")
        return {"status": "failure", "message": f"An unexpected error occurred: {e}"}

File: tool_scrape_url.py (C:\Users\DELL\Desktop\openAIF_frontend\Gemini_george_bot\GEORGE_bot_v0\tools\web\tool_scrape_url.py)
Content (14980 characters):
import os
import logging
from urllib.parse import urljoin, urlparse
import json
import hashlib
import mimetypes
import time
import requests
from typing import Dict, Optional
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
import concurrent.futures

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class SeleniumDriver:
    def __init__(self):
        self.options = Options()
        # Remove the --headless argument
        # self.options.add_argument('--headless')
        self.options.add_argument('--no-sandbox')
        self.options.add_argument('--disable-dev-shm-usage')
        self.options.add_argument('--disable-gpu')
        self.options.add_argument('--window-size=1920x1080')
        self.options.add_argument('--disable-notifications')
        self.options.add_argument('--disable-infobars')
        self.options.add_argument('--disable-extensions')
        self.options.add_experimental_option('prefs', {
            'download.default_directory': os.getcwd(),
            'download.prompt_for_download': False,
            'download.directory_upgrade': True,
            'safebrowsing.enabled': True
        })

    def __enter__(self):
        # Use a specific Chrome driver executable path (if needed)
        # driver_path = '/path/to/chromedriver'  # Replace with your actual path
        # self.driver = webdriver.Chrome(executable_path=driver_path, options=self.options)
        self.driver = webdriver.Chrome(options=self.options)
        return self.driver

    def __exit__(self, exc_type, exc_val, exc_tb):
        if hasattr(self, 'driver'):
            self.driver.quit()


def tool_scrape_url(
        url: str,
        scrape_images: bool = False,
        scrape_text: bool = False,
        scrape_links: bool = False,
        save_images: bool = False,
        save_text: bool = False,
        save_links: bool = False,
        get_whole_page: bool = False,
        save_path: str = None,
        return_type: str = "all"  # Options: "all", "images", "text", "links", "html"
) -> dict:
    """
    Scrapes content from a URL based on specified parameters using Selenium and Chrome.
    Returns all scraping information (images, text, links, etc.).

    Args:
        url (str): The URL to scrape.
        scrape_images (bool): Whether to scrape images from the page.
        scrape_text (bool): Whether to scrape text content from the page.
        scrape_links (bool): Whether to scrape links from the page.
        save_images (bool): Whether to save scraped images locally.
        save_text (bool): Whether to save scraped text locally.
        save_links (bool): Whether to save scraped links locally.
        get_whole_page (bool): Whether to get the entire HTML content.
        save_path (str): Base path for saving files (default: current directory).
        return_type (str): What type of content to return in the response. Options: "all", "images", "text", "links", "html"

    Returns:
        dict: A dictionary containing:
            - status: "success" or "failure"
            - message: Status message
            - data: Dictionary containing scraped content based on return_type
            - scraped_images: List of image data (URLs, alt text, etc.)
            - scraped_text_elements: List of text element data (type, content, etc.)
            - scraped_links: List of link data (URLs, text, etc.)
            - saved_images: List of saved image paths
            - saved_text: Whether text was saved to a file
            - saved_links: Whether links were saved to a file
            - html_saved: Whether the entire HTML content was saved
    """
    try:
        # Initialize result dictionary
        result = {
            "status": "success",
            "message": "Scraping completed successfully",
            "data": {},
            "scraped_images": [],
            "scraped_text_elements": [],
            "scraped_links": [],
            "saved_images": [],
            "saved_text": False,
            "saved_links": False,
            "html_saved": False
        }

        # Set default save path if none provided
        save_path = save_path or os.getcwd()
        os.makedirs(save_path, exist_ok=True)

        # Headers for requests (add more if needed)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36'
        }

        with SeleniumDriver() as driver:
            # Navigate to the URL with timeout and wait for page load
            driver.set_page_load_timeout(30)
            driver.get(url)

            # Wait for the page to be fully loaded
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )

            # Allow dynamic content to load (adjust based on page characteristics)
            time.sleep(2)  # Adjust based on page characteristics

            # Scroll to load lazy content
            last_height = driver.execute_script("return document.body.scrollHeight")
            while True:
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(1)
                new_height = driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    break
                last_height = new_height

            # Get the page source after JavaScript execution
            page_source = driver.page_source

            # Get whole page HTML if requested
            if get_whole_page:
                result["data"]["html"] = page_source
                if save_text:
                    html_path = os.path.join(save_path, "page.html")
                    with open(html_path, 'w', encoding='utf-8') as f:
                        f.write(page_source)
                    logger.info(f"Saved HTML to {html_path}")
                    result["html_saved"] = True

            # Scrape images
            if scrape_images:
                images = []
                image_elements = driver.find_elements(By.TAG_NAME, "img")
                result["scraped_images"] = [
                    {
                        "url": element.get_attribute('src') or '',
                        "alt": element.get_attribute('alt') or '',
                        "title": element.get_attribute('title') or '',
                        "width": element.get_attribute('width') or '',
                        "height": element.get_attribute('height') or '',
                        "natural_width": element.get_property('naturalWidth'),
                        "natural_height": element.get_property('naturalHeight'),
                        "is_displayed": element.is_displayed()
                    }
                    for element in image_elements
                ]

                def process_image(element):
                    try:
                        img_url = element.get_attribute('src')
                        if not img_url:
                            return None

                        if save_images and img_url.startswith(('http://', 'https://')):
                            try:
                                response = requests.get(img_url, headers=headers, timeout=10)
                                if response.status_code == 200:
                                    content_type = response.headers.get('content-type', '')
                                    if content_type.startswith('image/'):
                                        img_name = hashlib.md5(response.content).hexdigest()[:8]
                                        ext = mimetypes.guess_extension(content_type) or '.jpg'
                                        img_path = os.path.join(save_path, "images", f"{img_name}{ext}")
                                        os.makedirs(os.path.dirname(img_path), exist_ok=True)

                                        with open(img_path, 'wb') as f:
                                            f.write(response.content)
                                        result["saved_images"].append(img_path)
                                        logger.info(f"Saved image to {img_path}")
                            except Exception as e:
                                logger.error(f"Failed to save image {img_url}: {str(e)}")

                        return img_url
                    except Exception as e:
                        logger.error(f"Failed to process image element: {str(e)}")
                        return None

                # Process images concurrently
                with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                    image_urls = list(filter(None, executor.map(process_image, image_elements)))
                    images.extend(image_urls)

                result["data"]["images"] = images

            # Scrape text with JavaScript-rendered content
            if scrape_text:
                text_content = []
                text_elements = driver.find_elements(By.CSS_SELECTOR,
                                                    'p, h1, h2, h3, h4, h5, h6, article, section, main, [role="main"], [role="article"]')
                result["scraped_text_elements"] = [
                    {
                        "type": element.tag_name,
                        "content": element.text.strip(),
                        "html_classes": element.get_attribute('class') or '',
                        "id": element.get_attribute('id') or '',
                        "is_visible": element.is_displayed(),
                        "location": element.location
                    }
                    for element in text_elements
                ]

                for element in text_elements:
                    try:
                        text = element.text.strip()
                        if text:
                            text_content.append({
                                "type": element.tag_name,
                                "content": text,
                                "html_classes": element.get_attribute('class') or '',
                                "id": element.get_attribute('id') or '',
                                "is_visible": element.is_displayed(),
                                "location": element.location
                            })
                    except Exception as e:
                        logger.error(f"Failed to process text element: {str(e)}")

                result["data"]["text"] = text_content

                if save_text:
                    text_path = os.path.join(save_path, "content.txt")
                    with open(text_path, 'w', encoding='utf-8') as f:
                        for item in text_content:
                            f.write(f"{item['type'].upper()}:\n")
                            if item['id']:
                                f.write(f"ID: {item['id']}\n")
                            if item['html_classes']:
                                f.write(f"Classes: {item['html_classes']}\n")
                            f.write(f"Content: {item['content']}\n")
                            f.write(f"Visible: {item['is_visible']}\n\n")
                    logger.info(f"Saved text content to {text_path}")
                    result["saved_text"] = True

            # Scrape links including JavaScript-generated ones
            if scrape_links:
                links = []
                base_domain = urlparse(url).netloc
                link_elements = driver.find_elements(By.TAG_NAME, "a")
                result["scraped_links"] = [
                    {
                        "url": element.get_attribute('href') or '',
                        "text": element.text.strip(),
                        "title": element.get_attribute('title') or '',
                        "is_internal": urlparse(element.get_attribute('href') or '').netloc == base_domain,
                        "rel": element.get_attribute('rel') or '',
                        "target": element.get_attribute('target') or '',
                        "is_visible": element.is_displayed(),
                        "location": element.location
                    }
                    for element in link_elements
                ]

                for element in link_elements:
                    try:
                        href = element.get_attribute('href')
                        if href:
                            links.append({
                                "url": href,
                                "text": element.text.strip(),
                                "title": element.get_attribute('title') or '',
                                "is_internal": urlparse(href).netloc == base_domain,
                                "rel": element.get_attribute('rel') or '',
                                "target": element.get_attribute('target') or '',
                                "is_visible": element.is_displayed(),
                                "location": element.location
                            })
                    except Exception as e:
                        logger.error(f"Failed to process link element: {str(e)}")

                result["data"]["links"] = links

                if save_links:
                    links_path = os.path.join(save_path, "links.json")
                    with open(links_path, 'w', encoding='utf-8') as f:
                        json.dump(links, f, indent=2)
                    logger.info(f"Saved links to {links_path}")
                    result["saved_links"] = True

            # Filter return data based on return_type
            if return_type != "all":
                if return_type in result["data"]:
                    result["data"] = {return_type: result["data"][return_type]}
                else:
                    result["message"] += f" (Requested return_type '{return_type}' not found in scraped data)"

        return result

    except TimeoutException as e:
        error_msg = f"Page load timeout: {str(e)}"
        logger.error(error_msg)
        return {"status": "failure", "message": error_msg, "data": {}}
    except WebDriverException as e:
        error_msg = f"Selenium WebDriver error: {str(e)}"
        logger.error(error_msg)
        return {"status": "failure", "message": error_msg, "data": {}}
    except Exception as e:
        error_msg = f"An unexpected error occurred: {str(e)}"
        logger.error(error_msg)
        return {"status": "failure", "message": error_msg, "data": {}}

## Directory Tree

├── chat_loop_bot.py
├── focus
    ├── create_focus.py
    └── focus.json
├── future_tools_for_future _integration
    ├── tool_create_memory.py
    └── tool_update_internal_state.py
├── images
├── inner_brain_settings
    └── internal_state.json
├── keys.py
├── readMe
├── tools
    ├── ai
        ├── tool_AI_REASONING.py
        ├── tool_update_focus.py
        └── __pycache__
    ├── memory
    ├── os
        ├── tool_execute_script.py
        ├── tool_get_directory_structure.py
        ├── tool_read_from_file.py
        ├── tool_save_to_file.py
        └── __pycache__
    └── web
        ├── tool_get_duckduckgo_links.py
        ├── tool_save_image_from_url.py
        ├── tool_scrape_url.py
        └── __pycache__
├── TOOL_MANAGER.py
└── __pycache__

